from pyspark.sql.functions import when, col
from tqdm import tqdm
import time
import logging

# Configure logging to write to a file
logging.basicConfig(filename='max_values_log.txt', level=logging.INFO)

start_time = time.time()

# Aliases for database, schema, and table names
DATABASE_NAME = 'my_database' # Replace with your database name
SCHEMA_NAME = 'my_schema'     # Replace with your schema name
TABLE_NAME = 'my_table'       # Replace with your table name

# Query to get table and column information for integer columns
info_schema_query = f"""
SELECT table_catalog, table_schema, table_name, column_name, data_type
FROM system.information_schema.columns
WHERE data_type IN ('INT')
AND table_catalog in ('{DATABASE_NAME}')
AND table_schema in ('{SCHEMA_NAME}')
AND table_name in ('{TABLE_NAME}')
"""

# Execute the query and get the result as a DataFrame
info_df = spark.sql(info_schema_query)

# Collect the schema, table, and column information
columns_info = info_df.collect()
total_columns = info_df.count()

# Check if columns_info is empty
if not columns_info:
    print("No integer columns found.")
else:
    # Function to get max value for a given table and column
    def get_max_value(table_catalog, table_schema, table_name, column_name):
        query = f"SELECT MAX({column_name}) as max_value FROM {table_catalog}.{table_schema}.{table_name}"
        result_df = spark.sql(query)
        max_value = result_df.collect()[0]['max_value']
        return max_value

    # Function to get ther col schema for a given table and column
    def get_col_Schema(table_catalog, table_schema, table_name, column_name):
        # Create the schema query
        schema_query = f"DESCRIBE {table_catalog}.{table_schema}.{table_name}"

        # Execute the query and get the schema as a Spark DataFrame
        schema_df = spark.sql(schema_query)

        # Filter the DataFrame for the specific column
        col_schema_df = schema_df.filter(schema_df.col_name == column_name)

        # Extract the data type for the column
        col_schema = col_schema_df.collect()[0]['data_type']

        return col_schema

    # Iterate over the columns and get the max value for each integer column
    max_values = []
    update_frequency = 10  # Update progress bar every 10 iterations

    with tqdm(total=total_columns, desc="Processing Rows", unit="Rows") as pbar:
        for i, row in enumerate(columns_info, start=1):
            catalog = row['table_catalog']
            schema = row['table_schema']
            table = row['table_name']
            column = row['column_name']
            data_type = row['data_type']
            try:
                desc_data_type = get_col_Schema(catalog, schema, table, column)
                max_value = get_max_value(catalog, schema, table, column)
                max_values.append((catalog, schema, table, column, data_type, desc_data_type, max_value))

                # Update progress bar less frequently
                if i % update_frequency == 0 or i == total_columns:
                    pbar.update(min(update_frequency, total_columns - pbar.n))
                    elapsed_time = time.time() - start_time
                    avg_time_per_iteration = elapsed_time / i
                    remaining_time = avg_time_per_iteration * (total_columns - i)
                    pbar.set_postfix(remaining=f"{remaining_time / 60:.2f} min")

                    # Log the progress instead of printing
                    logging.info(f"Iteration {i}/{total_columns} - Estimated remaining time: {remaining_time / 60:.2f} minutes")

            except Exception as e:
                logging.error(f"Error processing {catalog}.{schema}.{table}.{column}: {e}")

    # Check if max_values is empty before creating DataFrame
    if not max_values:
        print("No max values found.")
    else:
        # Convert the results to a DataFrame for better visualization
        max_values_df = spark.createDataFrame(max_values, ["catalog", "schema", "table", "column", "data_type", "desc_data_type", "max_value"])

        # Add risk_level column based on max_value
        max_values_df = max_values_df.withColumn(
            "risk_level",
            when(col("max_value") > 2000000000, "High Risk")
            .when(col("max_value") > 1500000000, "Medium Risk")
            .otherwise("Low Risk")
        )

        display(max_values_df)

# Calculate and print the total elapsed time
elapsed_time = time.time() - start_time
print(f"Total elapsed time: {elapsed_time / 60:.2f} minutes")